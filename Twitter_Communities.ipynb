{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social Media Analytics Project 8 - Community Detection in a Twitter Network\n",
    "===\n",
    "Goloviatinski Sergiy, Herbelin Ludovic <br />\n",
    "MCS 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "from math import sqrt, log\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COMBINED_PATH = 'data/twitter_combined.txt'\n",
    "DATA_OTHERS = 'data/twitter/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 81306\n",
      "Number of edges : 1342310\n"
     ]
    }
   ],
   "source": [
    "original_G = nx.Graph()\n",
    "\n",
    "edges = nx.read_edgelist(DATA_COMBINED_PATH)\n",
    "\n",
    "original_G.add_edges_from(edges.edges())\n",
    "\n",
    "print(f\"Number of nodes : {len(original_G.nodes)}\")\n",
    "print(f\"Number of edges : {len(original_G.edges())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the size of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(G,n):\n",
    "    \n",
    "    node=random.choice(list(G.nodes))\n",
    "    \n",
    "    visited = set()\n",
    "    visited.add(node)\n",
    "    \n",
    "    while len(visited)<n:\n",
    "        node=random.choice(list(G.neighbors(node)))\n",
    "        visited.add(node)\n",
    "    \n",
    "    visited=list(visited)\n",
    "    \n",
    "    # we copy the graph because some attributes are shared with the original graph after calling subgraph method\n",
    "    G=G.copy()\n",
    "    return G.subgraph(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 10\n",
      "Number of edges : 20\n"
     ]
    }
   ],
   "source": [
    "# Reduce the graph size, with random walk\n",
    "N_NODES = 10\n",
    "\n",
    "G = random_walk(original_G,N_NODES)\n",
    "\n",
    "print(f\"Number of nodes : {len(G.nodes)}\")\n",
    "print(f\"Number of edges : {len(G.edges())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Girvan-Newman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gn(G, n_iter):\n",
    "    G=G.copy()\n",
    "    n_nodes=len(G.nodes)\n",
    "    nodes_affected=set()\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        betweennesses=nx.edge_betweenness_centrality(G,normalized=False)\n",
    "        max_betweenness=max(betweennesses.values())\n",
    "        for edge, betweeness in betweennesses.items():\n",
    "            # to remove n edges that all have the same value which is equals to max\n",
    "            if betweeness == max_betweenness:\n",
    "                G.remove_edge(edge[0],edge[1])\n",
    "                nodes_affected.add(edge[0])\n",
    "                nodes_affected.add(edge[1])\n",
    "        \n",
    "    # hypothesis: each node affected by the edge removal will be the starting point for a community.\n",
    "    # create a dict with community id as key, and set of nodes in this community as value\n",
    "    communities=dict(zip(list(range(len(nodes_affected))),[set([node]) for node in nodes_affected]))\n",
    "    \n",
    "    duplicate_communities=True\n",
    "    \n",
    "    while duplicate_communities:\n",
    "        n_nodes_in_communities=0\n",
    "        # while all the original nodes are not affected to a community\n",
    "        while n_nodes_in_communities<n_nodes:\n",
    "            for _, community in communities.items():\n",
    "                # Random walk to add nodes to communities\n",
    "                node=random.choice(list(community))\n",
    "                try:\n",
    "                    community.add(random.choice(list(G.neighbors(node))))\n",
    "                except IndexError:\n",
    "                    # If a node has no neighbor, it's a community on its own\n",
    "                    pass\n",
    "\n",
    "            n_nodes_in_communities=len(set().union(*list(communities.values())))\n",
    "\n",
    "            \n",
    "        # Idea to elimite duplicate communities: a duplicate community is created if two starting nodes\n",
    "        # (in nodes_affected list) are reachable from one to another, to avoid duplicating communities, we restart\n",
    "        # the whole process after removing one of the two starting nodes which are in the same community\n",
    "        # from the nodes_affected list\n",
    "        duplicate_communities=False\n",
    "        for i in communities.keys():\n",
    "            for j in communities.keys():\n",
    "                # iterate through the \"matrix\" in a triangular way: avoid to compare a community with itself\n",
    "                # and avoid to compare B with A after we already compared A with B\n",
    "                if i>j:\n",
    "                    nodes_affected_size=len(nodes_affected)\n",
    "                    for node in communities[i]:\n",
    "                        if node in communities[j]:\n",
    "                            duplicate_communities=True\n",
    "                            # The goal is to remove a node that appears in 2 communities from the starting nodes\n",
    "                            if len(nodes_affected)<nodes_affected_size:\n",
    "                                break\n",
    "                            try:\n",
    "                                nodes_affected.remove(node)\n",
    "                            except KeyError:\n",
    "                                pass\n",
    "        \n",
    "        if duplicate_communities:\n",
    "            communities=dict(zip(list(range(len(nodes_affected))),[set([node]) for node in nodes_affected]))\n",
    "    \n",
    "    return communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vi_neighbors, vj_neighbors):\n",
    "    return len(vi_neighbors.intersection(vj_neighbors)) / sqrt(len(vi_neighbors) * len(vj_neighbors))\n",
    "\n",
    "def compute_cosine_sim(G, selected_nodes):\n",
    "    nodes_similarities = {}\n",
    "    \n",
    "    # TODO : optimize not to compute multiple times the same product maybe triangular matrix\n",
    "    for node in selected_nodes:\n",
    "        vi_neighbors = set(G[node])\n",
    "        for neighbor in vi_neighbors:\n",
    "            vj_neighbors = set(G[neighbor])\n",
    "            sim = cosine_sim(vi_neighbors, vj_neighbors)\n",
    "            nodes_similarities[(node, neighbor)] = sim\n",
    "    \n",
    "    return nodes_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adamic-Adar similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamic_adar_sim(G, vi_neighbors, vj_neighbors):\n",
    "    common_neighbors = vi_neighbors.intersection(vj_neighbors)\n",
    "    \n",
    "    # sum of 1 / log(nb of neighbors for each common neighbor to vi and vj)\n",
    "    return sum([1 / log(len(G[neighbor])) for neighbor in common_neighbors])\n",
    "        \n",
    "\n",
    "def compute_adamic_adar_sim(G, selected_nodes):\n",
    "    nodes_similarities = {}\n",
    "    \n",
    "    # TODO : optimize not to compute multiple times the same product maybe triangular matrix\n",
    "    for node in selected_nodes:\n",
    "        vi_neighbors = set(G[node])\n",
    "        for neighbor in vi_neighbors:\n",
    "            vj_neighbors = set(G[neighbor])\n",
    "            sim = adamic_adar_sim(G, vi_neighbors, vj_neighbors)\n",
    "            nodes_similarities[(node, neighbor)] = sim\n",
    "    \n",
    "    return nodes_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the clusters and evaluate with different values of iteration level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'59836995'},\n",
       " 1: {'25276905', '806283'},\n",
       " 2: {'16870573'},\n",
       " 3: {'130574973', '16273438', '18393753', '23133578', '25147689', '62686586'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations=2\n",
    "compute_gn(G,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'59836995'}\n",
      "{'62686586', '16273438', '18393753', '130574973', '25147689', '23133578'}\n",
      "{'16870573', '25276905', '806283'}\n"
     ]
    }
   ],
   "source": [
    "# to compare with method from networkx\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "\n",
    "comp = girvan_newman(G)\n",
    "\n",
    "for i in range(iterations-1):\n",
    "    next(comp)\n",
    "for com in next(comp):\n",
    "    print(com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the top K users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'59836995': 0.6, '62686586': 1.0, '25276905': 0.6, '16273438': 0.8, '18393753': 1.0, '130574973': 1.0, '16870573': 0.4, '25147689': 1.0, '806283': 0.6, '23133578': 1.0}\n",
      "10 nodes with highest degree : {'62686586': 1.0, '18393753': 1.0, '130574973': 1.0, '25147689': 1.0, '23133578': 1.0, '16273438': 0.8, '59836995': 0.6, '25276905': 0.6, '806283': 0.6, '16870573': 0.4}\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "if k > len(G):\n",
    "    print(f\"Warning : K chosen : {k} is higher than the number of nodes in the graph : {len(G)}\\n\")\n",
    "\n",
    "nodes_degrees = dict(G.degree())\n",
    "# normalize the node degrees using the max node degree\n",
    "max_deg = max(nodes_degrees.values())\n",
    "nodes_degrees = {node:deg /float(max_deg) for node, deg in nodes_degrees.items()}\n",
    "\n",
    "    \n",
    "# sort the node:degree dictionary\n",
    "nodes_degrees= {node: deg for node, deg in sorted(nodes_degrees.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "\n",
    "top_nodes = list(nodes_degrees)[:k]\n",
    "top_nodes_dict = {node:nodes_degrees[node] for node in top_nodes}\n",
    "print(f\"{k} nodes with highest degree : {top_nodes_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most similar nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c799e161e1dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msimilarity_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarity_func\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimilarities_tested\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtop_nodes_sims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarity_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmost_similar_pair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_nodes_sims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtop_nodes_sims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Most similar nodes using function : {similarity_label} are {most_similar_pair} with similarity value : {top_nodes_sims[most_similar_pair]:.3f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "similarities_tested = {\n",
    "    'cosine':compute_cosine_sim,\n",
    "    'adamic-adar':compute_adamic_adar_sim,\n",
    "}\n",
    "\n",
    "\n",
    "for similarity_label, similarity_func in similarities_tested.items():\n",
    "    top_nodes_sims = similarity_func(G, top_nodes)\n",
    "    most_similar_pair = max(top_nodes_sims, key=top_nodes_sims.get)\n",
    "    \n",
    "    print(f\"Most similar nodes using function : {similarity_label} are {most_similar_pair} with similarity value : {top_nodes_sims[most_similar_pair]:.3f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
